---
# Variables listed here are applicable to all

# some operations could be time consuming and users may want to skip them
# totally even though they are idempotent
step_switch:
  nic: no
  hostname: yes
  selinux: yes
  ssh_key: yes
  partition: yes
  repo: yes
  ntp: yes

# each host must set ceph_network_ip and vm_network_ip
network_interfaces:
  - name: ceph_network
    nic: eth0
    prefix: 24
    network: 10.21.1.0
    use_bonding: no
    ip_addr_ref: ceph_network_ip

  - name: vm_network
    nic: eth1
    prefix: 24
    network: 10.21.1.0
    use_bonding: no
    ip_addr_ref: vm_network_ip
    
# switch used to control ceph type
# valid values: S_HDD, E_HDD, SSD
# S_HDD: hdd-based cluster, provide large volume disks
# E_HDD: hdd-based cluster, provide large, and higher performance disk 
# SSD: ssd-based cluster, provide highest performance disk
#
ClUSTER_TYPE: "S_HDD"

#when deploy openstack we need admin keyring if cephx is enabled
#openstack glance-api and nova-compute support only one backend,
#so it should be set to "yes" only once;if you wanna glance-api and nova-compute
#use this "ClUSTER_TYPE" backend, set it to yes;
COPY_KEYRING: no

# Below is variables used to create local repo
#
INTERNAL_REPO_ENABLED: yes

INTERNAL_CENTOS_REPO_HOST: "10.21.1.10"
INTERNAL_CEPH_REPO_HOST: "123.59.199.64"
#when deploy ceph use public network internal repo,
#we need uncomment sslclientcert and sslclientkey,also do it in repo.yml file
#also edit "http" to https,

INTERNAL_REPO_MIRRORS:
  - name: epel
    file: epel
    # libboost lttng cannot be found in internal repo
    #baseurl: "{{ INTERNAL_REPO_HOST }}/epel/$releasever/$basearch/"
    baseurl: "http://{{ INTERNAL_CENTOS_REPO_HOST }}/epel/$releasever/$basearch/"
    #sslclientcert: "/etc/pki/internal/yum.pem"
    #sslclientkey: "/etc/pki/internal/yum-key.pem"
  - name: base
    file: CentOS-Base
    baseurl: "http://{{ INTERNAL_CENTOS_REPO_HOST }}/centos/$releasever/os/$basearch/" 
    #sslclientcert: "/etc/pki/internal/yum.pem"
    #sslclientkey: "/etc/pki/internal/yum-key.pem"
  - name: updates
    file: CentOS-Base
    baseurl: "http://{{ INTERNAL_CENTOS_REPO_HOST }}/centos/$releasever/updates/$basearch/" 
    #sslclientcert: "/etc/pki/internal/yum.pem"
    #sslclientkey: "/etc/pki/internal/yum-key.pem"
  - name: extras
    file: CentOS-Base
    baseurl: "http://{{ INTERNAL_CENTOS_REPO_HOST }}/centos/$releasever/extras/$basearch/"   
    #sslclientcert: "/etc/pki/internal/yum.pem"
    #sslclientkey: "/etc/pki/internal/yum-key.pem"
  - name: Ceph
    file: ceph-jewel
    baseurl: "http://{{ INTERNAL_CEPH_REPO_HOST }}/ceph-rpm/jemalloc/noarch/"
    priority: 1 
    #sslclientcert: "/etc/pki/internal/yum.pem"
    #sslclientkey: "/etc/pki/internal/yum-key.pem"
  - name: Ceph-noarch
    file: ceph-jewel
    baseurl: "http://{{ INTERNAL_CEPH_REPO_HOST }}/ceph-rpm/jemalloc/noarch/"
    priority: 1   
    #sslclientcert: "/etc/pki/internal/yum.pem"
    #sslclientkey: "/etc/pki/internal/yum-key.pem"
  - name: Ceph-x86_64
    file: ceph-jewel
    baseurl: "http://{{ INTERNAL_CEPH_REPO_HOST }}/ceph-rpm/jemalloc/x86_64/"
    priority: 1 
    #sslclientcert: "/etc/pki/internal/yum.pem"
    #sslclientkey: "/etc/pki/internal/yum-key.pem"
  - name: Ceph-source
    file: ceph-jewel
    baseurl: "http://{{ INTERNAL_CEPH_REPO_HOST }}/ceph-rpm/jemalloc/SRPMS/"
    priority: 1 
    #sslclientcert: "/etc/pki/internal/yum.pem"
    #sslclientkey: "/etc/pki/internal/yum-key.pem"



# Below is repo for Chinac only
CHINAC_REPO_ENABLED: no
CHINAC_REPO_MIRROS:
  - name: centos-openstack-mitaka
    file: CentOS-OpenStack-mitaka
    baseurl: "https://125.208.28.24/repos/openstack/openstack-mitaka/base"
    sslclientcert: "/etc/pki/chinac/yum.pem"
    sslclientkey: "/etc/pki/chinac/yum-key.pem"
    #- name: centos-openstack-mitaka-dev
    #- file: CentOS-OpenStack-mitaka
    #- baseurl: "http://172.16.7.201/openstack/build/Dev/mitaka-chinac/latest"
    #- sslclientcert: ""
    #- sslclientkey: ""

PIP_REPO_MIRROR_HOST: pypi.mirrors.ustc.edu.cn

#if use 1.5.34 will happen gatherkeys 'allow mds *' problem
CEPH_DPLOY_RPM_URL: http://172.16.24.35/ceph-rpm/jemalloc/noarch/ceph-deploy-1.5.34-0.noarch.rpm
DOWNLOAD_FOLDER: /opt/download

CEPH_PACKAGE_URL: ftp://openstacksoft:Aaopenstacksoft@125.208.28.24/ceph6.tar.gz

# in order to support multiple osds on single disk, partitions maybe used for data store,
# so from now, paritions are used for two types: OSD or journal
# for OSD type, the name value should start with "data", such as data01 or data02
# for journal type, the name value should start with "journal", such as journal01 or journal02
disk_partition_layouts:
  disk_partition_layout1:
    - device: vdb
      partitions:
      - percent: [4096s, 100%]
        name: journal01
        filesystem: xfs
 
# if you don't have a seperate journal, please set steps.partition to no
ceph_osd_layouts:
  # 1 SSD is sed as SATA journal
  ceph_osd_layout_1:
    - device: vda
      use_cache: no
      cache: vdb1
      use_type: yes
      type: hdd
    - device: vdb
      use_cache: no
      cache: vdb1
      use_type: yes
      type: hdd
    - device: vdc
      use_cache: no
      cache: vdb2
      use_type: yes
      type: hdd
   
ceph_osd_layout_map:
  tt-master:
    partition: disk_partition_layout1
    osd: ceph_osd_layout_1
  tt-slave1:
    partition: disk_partition_layout1
    osd: ceph_osd_layout_1
 
ceph_management_dir: /root/ceph-management/
 
ceph_osd_layouts:
  # 6 SATA + 3 SSD for OSDs
  # 1 SSD is sed as SATA journal
  ceph_osd_layout_1:
    - device: vda
      use_cache: no
      cache: vdb1
      use_type: yes
      type: hdd
    - device: vdb
      use_cache: no
      cache: vdb2
      use_type: yes
      type: hdd
    - device: vdc
      use_cache: no
      cache: vdb2
      use_type: yes
      type: hdd
   
ceph_osd_layout_map:
  tt-master:
    partition: disk_partition_layout1
    osd: ceph_osd_layout_1
  tt-slave1:
    partition: disk_partition_layout1
    osd: ceph_osd_layout_1
 
ceph_management_dir: /root/ceph-management/

# vim: ft=yaml:shiftwidth=2:tabstop=2:expandtab:softtabstop=2
